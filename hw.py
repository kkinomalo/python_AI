# get_page_advanced_data_to_file.py (íŒŒì¼ ì´ë¦„ í†µì¼)

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
from bs4.element import Comment # HTML ì£¼ì„ ì¶”ì¶œì„ ìœ„í•´ í•„ìš”
import time
import os
import requests # ì›¹ ì„œë²„ ì‘ë‹µ ì‹œê°„ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import json # JSON ë°ì´í„°ë¥¼ ê¹”ë”í•˜ê²Œ ì¶œë ¥í•˜ê¸° ìœ„í•´ ì¶”ê°€

def get_chrome_driver(headless=True):
    """
    Chrome WebDriverë¥¼ ì„¤ì •í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    headless=Trueë¡œ ì„¤ì •í•˜ë©´ ë¸Œë¼ìš°ì € ì°½ì´ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    options = webdriver.ChromeOptions()
    if headless:
        options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36')
    return webdriver.Chrome(options=options)

def extract_and_save_advanced_page_data(url, output_filename="DLRJ.txt", headless=False, wait_time=15):
    """
    ì§€ì •ëœ URLì˜ ì›¹ í˜ì´ì§€ì—ì„œ HTML ì†ŒìŠ¤, ì£¼ìš” ìš”ì†Œë“¤,
    ê·¸ë¦¬ê³  ì¶”ê°€ì ì¸ ê¸°ìˆ  ì •ë³´ë“¤ì„ ì¶”ì¶œí•˜ì—¬ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    driver = None
    exception_occurred = False # ì˜ˆì™¸ ë°œìƒ ì—¬ë¶€ë¥¼ ì¶”ì í•˜ëŠ” í”Œë˜ê·¸

    output_buffer = []
    output_buffer.append("=" * 70)
    output_buffer.append(f"âœ¨ ì›¹ í˜ì´ì§€ ê³ ê¸‰ ë°ì´í„° ì¶”ì¶œ ê²°ê³¼ - URL: {url} âœ¨")
    output_buffer.append("=" * 70)

    try:
        # --- 1. ì›¹ ì„œë²„ ì‘ë‹µ ì‹œê°„ (í•‘ê³¼ ìœ ì‚¬í•œ ê°œë…) ---
        try:
            response = requests.get(url, timeout=10) # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
            server_response_time = round(response.elapsed.total_seconds() * 1000, 2) # ë°€ë¦¬ì´ˆ ë‹¨ìœ„
            http_status_code = response.status_code
            output_buffer.append(f"\n--- ğŸŒ ì›¹ ì„œë²„ ì‘ë‹µ ë° ìƒíƒœ ---")
            output_buffer.append(f"HTTP ìƒíƒœ ì½”ë“œ: {http_status_code}")
            output_buffer.append(f"ì„œë²„ ì‘ë‹µ ì‹œê°„ (Requests): {server_response_time} ms\n")
        except requests.exceptions.Timeout:
            output_buffer.append("\n--- ğŸŒ ì›¹ ì„œë²„ ì‘ë‹µ ë° ìƒíƒœ ---")
            output_buffer.append("ì„œë²„ ì‘ë‹µ ì‹œê°„ ì¸¡ì • ì‹¤íŒ¨: ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (Timeout)")
        except requests.exceptions.RequestException as e:
            output_buffer.append("\n--- ğŸŒ ì›¹ ì„œë²„ ì‘ë‹µ ë° ìƒíƒœ ---")
            output_buffer.append(f"ì„œë²„ ì‘ë‹µ ì‹œê°„ ì¸¡ì • ì‹¤íŒ¨: ìš”ì²­ ì˜¤ë¥˜ - {e}")
        output_buffer.append("\n")

        # --- Seleniumì„ í†µí•œ í˜ì´ì§€ ì ‘ì† ë° ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ---
        print(f"ì›¹ ë“œë¼ì´ë²„ë¥¼ ì‹œì‘í•˜ê³  '{url}' í˜ì´ì§€ì— ì ‘ì†í•©ë‹ˆë‹¤. (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ: {headless})")
        driver = get_chrome_driver(headless=headless)
        start_time_selenium_load = time.time() # Selenium ë¡œë”© ì‹œì‘ ì‹œê°„ ê¸°ë¡
        driver.get(url)

        print(f"í˜ì´ì§€ ë¡œë”©ì„ ìµœëŒ€ {wait_time}ì´ˆ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...")
        WebDriverWait(driver, wait_time).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        end_time_selenium_load = time.time() # Selenium ë¡œë”© ì™„ë£Œ ì‹œê°„ ê¸°ë¡
        selenium_load_duration = round((end_time_selenium_load - start_time_selenium_load), 2)
        print(f"í˜ì´ì§€ ë¡œë”© ì™„ë£Œ (Selenium ì†Œìš” ì‹œê°„: {selenium_load_duration}ì´ˆ).")

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        output_buffer.append("\n--- â±ï¸ í˜ì´ì§€ ë¡œë”© ì§€í‘œ (Selenium ê¸°ì¤€) ---")
        output_buffer.append(f"Selenium í˜ì´ì§€ ë¡œë”© ì‹œê°„: {selenium_load_duration} ì´ˆ\n")

        # --- ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ---
        title = soup.title.text.strip() if soup.title else "ì œëª© ì—†ìŒ"
        output_buffer.append(f"\n--- ğŸ“Œ í˜ì´ì§€ ì œëª© ---\n{title}\n")

        meta_description = soup.find('meta', attrs={'name': 'description'})
        description_content = meta_description['content'].strip() if meta_description and 'content' in meta_description.attrs else "ì„¤ëª… ì—†ìŒ"
        output_buffer.append(f"--- ğŸ“ ë©”íƒ€ ì„¤ëª… ---\n{description_content}\n")

        output_buffer.append("--- ğŸ“„ ì£¼ìš” ì„¹ì…˜ ì œëª© (H1, H2, H3) ---")
        headings = soup.find_all(['h1', 'h2', 'h3'])
        if headings:
            for heading in headings:
                output_buffer.append(f"<{heading.name.upper()}>: {heading.get_text(strip=True)}")
        else:
            output_buffer.append("ì£¼ìš” ì œëª© íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        output_buffer.append("\n")

        output_buffer.append("--- ğŸ“‘ ì£¼ìš” í…ìŠ¤íŠ¸ ë‹¨ë½ (ìƒìœ„ 5ê°œ) ---")
        paragraphs = soup.find_all('p')
        if paragraphs:
            for i, p in enumerate(paragraphs[:5]):
                text_content = p.get_text(strip=True)
                if text_content:
                    output_buffer.append(f"ë‹¨ë½ {i+1}: {text_content[:200]}...")
            if len(paragraphs) > 5:
                output_buffer.append(f"...ì™¸ {len(paragraphs) - 5}ê°œì˜ ë‹¨ë½ ë” ì¡´ì¬í•©ë‹ˆë‹¤.")
        else:
            output_buffer.append("ë‹¨ë½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        output_buffer.append("\n")

        output_buffer.append("--- ğŸ—’ï¸ ëª©ë¡ (UL/OL) ---")
        lists = soup.find_all(['ul', 'ol'])
        if lists:
            for list_tag in lists:
                list_type = "ìˆœì„œ ì—†ëŠ” ëª©ë¡" if list_tag.name == 'ul' else "ìˆœì„œ ìˆëŠ” ëª©ë¡"
                output_buffer.append(f"{list_type}:")
                list_items = list_tag.find_all('li')
                for item in list_items[:5]:
                    item_text = item.get_text(strip=True)
                    if item_text:
                        output_buffer.append(f"  - {item_text}")
                if len(list_items) > 5:
                    output_buffer.append(f"  ...ì™¸ {len(list_items) - 5}ê°œì˜ í•­ëª© ë”.")
        else:
            output_buffer.append("ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        output_buffer.append("\n")

        output_buffer.append("--- ğŸ”— í˜ì´ì§€ ë‚´ ëª¨ë“  ë§í¬ (ìƒìœ„ 10ê°œ) ---")
        links = soup.find_all('a', href=True)
        if links:
            for i, link in enumerate(links[:10]):
                link_text = link.get_text(strip=True)
                link_url = link['href']
                if link_text:
                    output_buffer.append(f"ë§í¬ {i+1}: í…ìŠ¤íŠ¸: '{link_text}', URL: '{link_url}'")
                else:
                    output_buffer.append(f"ë§í¬ {i+1}: URL: '{link_url}' (í…ìŠ¤íŠ¸ ì—†ìŒ)")
            if len(links) > 10:
                output_buffer.append(f"...ì™¸ {len(links) - 10}ê°œì˜ ë§í¬ ë” ì¡´ì¬í•©ë‹ˆë‹¤.")
        else:
            output_buffer.append("ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        output_buffer.append("\n")

        output_buffer.append("--- ğŸ–¼ï¸ í˜ì´ì§€ ë‚´ ëª¨ë“  ì´ë¯¸ì§€ (ìƒìœ„ 5ê°œ) ---")
        images = soup.find_all('img', src=True)
        if images:
            for i, img in enumerate(images[:5]):
                img_src = img['src']
                img_alt = img.get('alt', 'ëŒ€ì²´ í…ìŠ¤íŠ¸ ì—†ìŒ')
                output_buffer.append(f"ì´ë¯¸ì§€ {i+1}: SRC: '{img_src}', ALT: '{img_alt}'")
            if len(images) > 5:
                output_buffer.append(f"...ì™¸ {len(images) - 5}ê°œì˜ ì´ë¯¸ì§€ê°€ ë” ì¡´ì¬í•©ë‹ˆë‹¤.")
        else:
            output_buffer.append("ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        output_buffer.append("\n")

        # --- ì¶”ê°€ ì •ë³´ ì¶”ì¶œ ---

        # 8. HTML ì£¼ì„
        output_buffer.append("--- ğŸ’¬ HTML ì£¼ì„ ---")
        # BS4 Comment ê°ì²´ë¥¼ ì§ì ‘ ì„í¬íŠ¸í•˜ì—¬ ì‚¬ìš©
        comments = soup.find_all(string=lambda text: isinstance(text, Comment))
        if comments:
            for i, comment in enumerate(comments[:5]):
                output_buffer.append(f"ì£¼ì„ {i+1}: {comment.strip()[:100]}...")
            if len(comments) > 5:
                output_buffer.append(f"...ì™¸ {len(comments) - 5}ê°œì˜ ì£¼ì„ ë” ì¡´ì¬í•©ë‹ˆë‹¤.")
        else:
            output_buffer.append("HTML ì£¼ì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        output_buffer.append("\n")

        # 9. í¼ (Form) ì •ë³´
        output_buffer.append("--- ğŸ“ í¼ (Form) ì •ë³´ ---")
        forms = soup.find_all('form')
        if forms:
            for i, form in enumerate(forms):
                output_buffer.append(f"í¼ {i+1}:")
                output_buffer.append(f"  ì•¡ì…˜ (action): {form.get('action', 'ì—†ìŒ')}")
                output_buffer.append(f"  ë©”ì„œë“œ (method): {form.get('method', 'ì—†ìŒ')}")
                input_fields = form.find_all(['input', 'textarea', 'select'])
                if input_fields:
                    output_buffer.append("  ì…ë ¥ í•„ë“œ:")
                    for field in input_fields[:5]:
                        field_name = field.get('name', 'ì´ë¦„ ì—†ìŒ')
                        field_type = field.get('type', field.name)
                        output_buffer.append(f"    - íƒ€ì…: {field_type}, ì´ë¦„: {field_name}")
                    if len(input_fields) > 5:
                        output_buffer.append(f"    ...ì™¸ {len(input_fields) - 5}ê°œì˜ í•„ë“œ ë”.")
                else:
                    output_buffer.append("  ì…ë ¥ í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            output_buffer.append("í¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        output_buffer.append("\n")

        # 10. ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ë° ìŠ¤íƒ€ì¼ì‹œíŠ¸ ë§í¬
        output_buffer.append("--- âš™ï¸ ì™¸ë¶€ ë¦¬ì†ŒìŠ¤ (JS/CSS) ---")
        scripts = soup.find_all('script', src=True)
        styles = soup.find_all('link', rel='stylesheet', href=True)
        if scripts:
            output_buffer.append("ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼:")
            for i, script in enumerate(scripts[:5]):
                output_buffer.append(f"  {i+1}: {script['src']}")
            if len(scripts) > 5:
                output_buffer.append(f"  ...ì™¸ {len(scripts) - 5}ê°œì˜ ìŠ¤í¬ë¦½íŠ¸ ë”.")
        if styles:
            output_buffer.append("ìŠ¤íƒ€ì¼ì‹œíŠ¸ íŒŒì¼:")
            for i, style in enumerate(styles[:5]):
                output_buffer.append(f"  {i+1}: {style['href']}")
            if len(styles) > 5:
                output_buffer.append(f"  ...ì™¸ {len(styles) - 5}ê°œì˜ ìŠ¤íƒ€ì¼ì‹œíŠ¸ ë”.")
        if not scripts and not styles:
            output_buffer.append("ì™¸ë¶€ ìŠ¤í¬ë¦½íŠ¸ ë° ìŠ¤íƒ€ì¼ì‹œíŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        output_buffer.append("\n")

        # 11. íŠ¹ì • ë©”íƒ€ íƒœê·¸ (Charset, Viewport ë“±)
        output_buffer.append("--- ğŸ› ï¸ ê¸°íƒ€ ë©”íƒ€ íƒœê·¸ ---")
        meta_tags = soup.find_all('meta')
        useful_meta_tags = ['charset', 'viewport', 'keywords', 'author', 'generator', 'application-name']
        found_meta = False
        for meta in meta_tags:
            name = meta.get('name')
            http_equiv = meta.get('http-equiv')
            if name in useful_meta_tags or http_equiv:
                content = meta.get('content', '')
                if name:
                    output_buffer.append(f"  {name}: {content}")
                elif http_equiv:
                    output_buffer.append(f"  {http_equiv}: {content}")
                found_meta = True
            elif 'charset' in meta.attrs:
                output_buffer.append(f"  charset: {meta.get('charset', 'ì—†ìŒ')}")
                found_meta = True

        if not found_meta:
            output_buffer.append("íŠ¹ì • ë©”íƒ€ íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        output_buffer.append("\n")

        # 12. í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ì¿ í‚¤ ì •ë³´ (ì„¸ì…˜ ê´€ë¦¬ ë“± í™•ì¸)
        output_buffer.append("--- ğŸª ì¿ í‚¤ ì •ë³´ ---")
        cookies = driver.get_cookies()
        if cookies:
            output_buffer.append(json.dumps(cookies, indent=2, ensure_ascii=False))
        else:
            output_buffer.append("ì¿ í‚¤ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        output_buffer.append("\n")


        output_buffer.append("=" * 70)
        output_buffer.append("--- ğŸ“„ ì›ë³¸ HTML ì†ŒìŠ¤ ---")
        output_buffer.append("=" * 70)
        output_buffer.append(html)

        # --- íŒŒì¼ì— ì €ì¥ ---
        full_path = os.path.join(os.getcwd(), output_filename)
        with open(full_path, "w", encoding="utf-8") as f:
            f.write("\n".join(output_buffer))

        print(f"\nâœ… ë°ì´í„° ì¶”ì¶œ ë° '{output_filename}' íŒŒì¼ ì €ì¥ ì™„ë£Œ!")
        print(f"íŒŒì¼ ê²½ë¡œ: {full_path}")

    except TimeoutException as e: # ì˜ˆì™¸ ê°ì²´ eë¥¼ ëª…ì‹œì ìœ¼ë¡œ catch
        exception_occurred = True
        print("\n--- ğŸš¨ ì˜¤ë¥˜ ë°œìƒ ğŸš¨ ---")
        print(f"í˜ì´ì§€ ë¡œë”© ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤ ({wait_time}ì´ˆ). ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜, 'wait_time'ì„ ëŠ˜ë ¤ë³´ì„¸ìš”.")
        print(f"ì˜¤ë¥˜ ìƒì„¸: {e}")
        print("ë¸Œë¼ìš°ì €ë¥¼ ë‹«ì§€ ì•Šê³  ìœ ì§€í•©ë‹ˆë‹¤.")
    except WebDriverException as e: # ì˜ˆì™¸ ê°ì²´ eë¥¼ ëª…ì‹œì ìœ¼ë¡œ catch
        exception_occurred = True
        print("\n--- ğŸš¨ ì˜¤ë¥˜ ë°œìƒ ğŸš¨ ---")
        print(f"WebDriver ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        print("Chrome ë¸Œë¼ìš°ì €ì™€ ChromeDriverì˜ ë²„ì „ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        print("ë¸Œë¼ìš°ì €ë¥¼ ë‹«ì§€ ì•Šê³  ìœ ì§€í•©ë‹ˆë‹¤.")
    except Exception as e: # ì˜ˆì™¸ ê°ì²´ eë¥¼ ëª…ì‹œì ìœ¼ë¡œ catch
        exception_occurred = True
        print("\n--- ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ ğŸš¨ ---")
        print(f"ì˜¤ë¥˜ ë‚´ìš©: {e}")
        print("ì˜ˆìƒì¹˜ ëª»í•œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì½”ë“œ ë…¼ë¦¬ë‚˜ í™˜ê²½ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    finally:
        # exception_occurred í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ driver.quit() í˜¸ì¶œ ì—¬ë¶€ ê²°ì •
        if driver and not exception_occurred:
            print("ì›¹ ë“œë¼ì´ë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            driver.quit()
        elif driver and exception_occurred:
            print("ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì¸í•´ ë¸Œë¼ìš°ì €ë¥¼ ë‹«ì§€ ì•Šê³  ìœ ì§€í•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    # í¬ë¡¤ë§í•  ì›¹ì‚¬ì´íŠ¸ URL ì„¤ì •
    target_url = "https://gbsm.newrrow.com/csr-platform/home"

    # í•¨ìˆ˜ í˜¸ì¶œí•˜ì—¬ ë°ì´í„° ì¶”ì¶œ ë° íŒŒì¼ì— ì €ì¥
    # headless=Falseë¡œ ì„¤ì •í•˜ë©´ ë¸Œë¼ìš°ì € ì°½ì´ ì‹¤ì œë¡œ ì—´ë ¤ì„œ ê³¼ì •ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ë””ë²„ê¹…ì´ í•„ìš” ì—†ë‹¤ë©´ headless=Trueë¡œ ë³€ê²½í•˜ì„¸ìš”.
    extract_and_save_advanced_page_data(target_url, output_filename="DLRJ.txt", headless=False, wait_time=20)